# Regularization in Deep Neural Networks 

This assignment applies regularization techniques to prevent overfitting and improve the generalization of deep neural networks.  
It is part of **Week 2 (Course 2: Improving Deep Neural Networks – Hyperparameter Tuning, Regularization, and Optimization)** from the **Deep Learning Specialization** by **Andrew Ng** on Coursera.

##  Description

In this lab, I implemented two important regularization techniques:
- **L2 Regularization (Weight Decay)**: Penalizes large weights by adding an L2 penalty term to the cost function.
- **Dropout Regularization**: Randomly drops units during training to prevent co-adaptation of features.

Both techniques were used to reduce overfitting and improve model performance on unseen data.

### Key Concepts Covered:
- Bias-variance tradeoff
- L2 regularization (weight decay) for deep models
- Dropout regularization to reduce overfitting
- Effect of regularization on cost and accuracy
- Visualizing decision boundaries with and without regularization

##  Files Included

- `regularization_lab.ipynb`: Main notebook containing all experiments and comparisons
- `regularization_utils.py`: Helper functions for forward/backward propagation, cost calculation, and plotting
- `data/`: Synthetic datasets for classification tasks

> ⚠️ This repository includes only my own work and fully complies with Coursera’s Honor Code.

##  Tools Used

- Python 3
- NumPy
- Matplotlib
- Jupyter Notebook

##  Course Info

This assignment is part of:
> [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning)  
> Instructor: **Andrew Ng**  
> Course 2: Improving Deep Neural Networks  
> Week 2: Regularization

##  License

This repository is created for educational and portfolio purposes. Please avoid using it for direct submission on Coursera.

---

 If you're exploring deep learning generalization, feel free to star this repository!
